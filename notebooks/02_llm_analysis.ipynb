{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0291c1",
   "metadata": {},
   "source": [
    "## Notebook 02: LLM Analysis and Relevance Scoring\n",
    "\n",
    "**Objective**: Use GPT to analyze regulatory relevance of collected news articles\n",
    "\n",
    "**LLM Outputs (structured):**\n",
    "- Relevance score (0-10): How relevant to institutional investment regulation\n",
    "- Category: Type of regulation (monetary policy, banking, securities, etc.)\n",
    "- Impact level: High/Medium/Low impact on institutional investors\n",
    "- Key entities: Regulators and institutions mentioned\n",
    "- Summary: One-line description\n",
    "- Reasoning: Brief explanation of relevance score\n",
    "\n",
    "**Approach:**\n",
    "1. Load raw data from Notebook 01\n",
    "2. Design and test prompt on sample articles\n",
    "3. Batch process all articles with rate limiting\n",
    "4. Validate LLM outputs\n",
    "5. Save enriched dataset for dashboard\n",
    "\n",
    "**Model**: GPT-3.5-turbo (cost-effective, sufficient for classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ba695",
   "metadata": {},
   "source": [
    "### **Basic Library Import and Setup**\n",
    "\n",
    "**What I am doing:**\n",
    "- Import libraries for data processing and OpenAI API\n",
    "- Load environment variables and configuration\n",
    "- Initialize OpenAI client\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- Setup environment for LLM calls\n",
    "- Verify API credentials before processing\n",
    "- Load config parameters for analysis\n",
    "\n",
    "**Technical Note:** openai library, dotenv, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06156c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY == 'your_openai_key_here':\n",
    "    print(\"ERROR: OpenAI API key not found\")\n",
    "else:\n",
    "    print(\"OpenAI key loaded\")\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"OpenAI client initialized\")\n",
    "    \n",
    "print(f\"Model: {config['llm']['model']}\")\n",
    "print(f\"Analysis threshold: {config['analysis']['relevance_threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698f63b",
   "metadata": {},
   "source": [
    "### **Loading Raw Data From NB01**\n",
    "\n",
    "**What I am doing:**\n",
    "- Find and load the most recent raw data file from Notebook 01\n",
    "- Display basic statistics about the dataset\n",
    "- Verify data structure before LLM processing\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- Continue pipeline from data collection\n",
    "- Validate data loaded correctly before expensive API calls\n",
    "- Confirm article counts and structure\n",
    "\n",
    "**Technical Note:** pathlib for file handling, pandas for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585cd4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = Path('../data/raw')\n",
    "raw_files = sorted(raw_data_dir.glob('news_raw_*.json'))\n",
    "\n",
    "if not raw_files:\n",
    "    print(\"ERROR: No raw data files found. Run Notebook 01 first.\")\n",
    "else:\n",
    "    latest_file = raw_files[-1]\n",
    "    print(f\"Loading: {latest_file.name}\")\n",
    "    \n",
    "    df = pd.read_json(latest_file)\n",
    "    \n",
    "    print(f\"\\nDataset loaded: {len(df)} articles\")\n",
    "    print(f\"Markets: {df['market_name'].value_counts().to_dict()}\")\n",
    "    print(f\"Sources: {df['source_type'].value_counts().to_dict()}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nSample article:\")\n",
    "    print(f\"Title: {df.iloc[0]['title']}\")\n",
    "    print(f\"Market: {df.iloc[0]['market_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98f65b",
   "metadata": {},
   "source": [
    "### **LLM Analysis Prompt**\n",
    "\n",
    "**What I am doing:**\n",
    "- Create system prompt defining analyst role for Norges Bank context\n",
    "- Design structured JSON output with split summary (event + relevance)\n",
    "- Define analysis criteria focused on portfolio impact and regulatory signals\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- Separated event description and relevance provides clearer insights\n",
    "- Tailored prompts produce higher quality analysis\n",
    "- Structured output ensures reliable parsing for dashboard display\n",
    "\n",
    "**Technical Note:** String templating, JSON schema design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3170581",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a regulatory intelligence analyst for Norges Bank Investment Management, the world's largest sovereign wealth fund managing a $1.5 trillion global equity portfolio.\n",
    "\n",
    "Your role is to identify regulatory developments that could impact institutional investment operations.\n",
    "\n",
    "Evaluate news for:\n",
    "1. Portfolio Impact: Does this affect holdings, investment strategies, or market access?\n",
    "2. Systemic Risk: Does this signal broader market instability or regulatory shifts?\n",
    "3. Early Warning: Is this an emerging regulatory trend before formal implementation?\n",
    "\n",
    "Prioritize: Central bank policy, securities regulation, cross-border investment rules, and systemic financial regulation.\n",
    "\n",
    "Assign LOW scores (0-3) to: Company-specific news, non-financial regulation, opinion pieces without regulatory substance.\"\"\"\n",
    "\n",
    "def create_analysis_prompt(article):\n",
    "    \"\"\"Generate analysis prompt for an article\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze this article for regulatory relevance to institutional investors:\n",
    "\n",
    "Title: {article['title']}\n",
    "Description: {article.get('description', 'N/A')}\n",
    "Source: {article.get('source_name', 'Unknown')}\n",
    "Market: {article['market_name']}\n",
    "\n",
    "Return analysis in this exact JSON format:\n",
    "{{\n",
    "  \"relevance_score\": <0-10 integer, where 10 is critical regulatory impact>,\n",
    "  \"impact_level\": \"<high/medium/low>\",\n",
    "  \"category\": \"<monetary_policy/banking_regulation/securities_regulation/market_infrastructure/central_banking/cross_border_investment/other>\",\n",
    "  \"key_regulators\": [\"<primary regulator>\", \"<secondary if applicable>\"],\n",
    "  \"what_happened\": \"<Brief overview of the regulatory event or announcement>\",\n",
    "  \"why_relevant\": \"<Why this matters specifically for institutional investors like Norges Bank>\",\n",
    "  \"confidence\": \"<high/medium/low - clarity of regulatory significance>\"\n",
    "}}\n",
    "\n",
    "Be concise and actionable. Focus on investment decision-making intelligence.\"\"\"\n",
    "    \n",
    "    return user_prompt\n",
    "\n",
    "print(\"Prompt design complete\")\n",
    "print(f\"\\nCategories: {config['analysis']['categories']}\")\n",
    "print(f\"Impact levels: {config['analysis']['impact_levels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bdcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing prompt on sample articles\\n\")\n",
    "\n",
    "test_articles = df.sample(3, random_state=42)\n",
    "\n",
    "for idx, article in test_articles.iterrows():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST ARTICLE: {article['title'][:80]}...\")\n",
    "    print(f\"Market: {article['market_name']}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=config['llm']['model'],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": create_analysis_prompt(article)}\n",
    "            ],\n",
    "            temperature=config['llm']['temperature'],\n",
    "            max_tokens=config['llm']['max_tokens'],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        analysis = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        print(\"LLM Analysis:\")\n",
    "        print(f\"Relevance Score: {analysis.get('relevance_score')}/10\")\n",
    "        print(f\"Impact Level: {analysis.get('impact_level')}\")\n",
    "        print(f\"Category: {analysis.get('category')}\")\n",
    "        print(f\"Key Regulators: {analysis.get('key_regulators')}\")\n",
    "        print(f\"\\nWhat Happened: {analysis.get('what_happened')}\")\n",
    "        print(f\"Why Relevant: {analysis.get('why_relevant')}\")\n",
    "        print(f\"Confidence: {analysis.get('confidence')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "\n",
    "print(\"\\n\\nPrompt test complete. Review outputs before full batch processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(analysis, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d93ee75",
   "metadata": {},
   "source": [
    "### **Batch Processing**\n",
    "\n",
    "**What I am doing:**\n",
    "- Loop through all 652 articles and call OpenAI API for each\n",
    "- Add rate limiting delays to avoid API throttling\n",
    "- Store analysis results with error handling for failed articles\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- Generate relevance scores and categorization for entire dataset\n",
    "- Progress bar shows estimated completion time (~5-7 minutes)\n",
    "- Error handling ensures one bad article doesn't stop entire batch\n",
    "\n",
    "**Technical Note:** tqdm for progress tracking, time.sleep for rate limiting, try-except for error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting batch analysis of {len(df)} articles\")\n",
    "print(f\"Estimated time: {len(df) * 1.5 / 60:.1f} minutes\\n\")\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "for idx, article in tqdm(df.iterrows(), total=len(df), desc=\"Analyzing articles\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=config['llm']['model'],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": create_analysis_prompt(article)}\n",
    "            ],\n",
    "            temperature=config['llm']['temperature'],\n",
    "            max_tokens=config['llm']['max_tokens'],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        analysis = json.loads(response.choices[0].message.content)\n",
    "        analysis['article_index'] = idx\n",
    "        results.append(analysis)\n",
    "        \n",
    "        time.sleep(config['analysis']['rate_limit_delay'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors.append({'index': idx, 'title': article['title'], 'error': str(e)})\n",
    "        results.append({\n",
    "            'article_index': idx,\n",
    "            'relevance_score': 0,\n",
    "            'impact_level': 'low',\n",
    "            'category': 'other',\n",
    "            'key_regulators': [],\n",
    "            'what_happened': 'Analysis failed',\n",
    "            'why_relevant': 'N/A',\n",
    "            'confidence': 'low'\n",
    "        })\n",
    "\n",
    "print(f\"\\n\\nAnalysis complete!\")\n",
    "print(f\"Successful: {len(results) - len(errors)}\")\n",
    "print(f\"Errors: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nFirst few errors:\")\n",
    "    for err in errors[:3]:\n",
    "        print(f\"  - {err['title'][:60]}... : {err['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32e268",
   "metadata": {},
   "source": [
    "What I am doing:\n",
    "\n",
    "Convert LLM analysis results to DataFrame\n",
    "Merge analysis with original article data on index\n",
    "Verify merge completed successfully with all expected columns\n",
    "\n",
    "Why I'm doing this:\n",
    "\n",
    "Creates single enriched dataset with articles and analysis\n",
    "Enables filtering by relevance scores and categories\n",
    "Prepares data structure for dashboard consumption\n",
    "\n",
    "Technical Note: pandas DataFrame operations, merge validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19426a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging analysis with article data\\n\")\n",
    "\n",
    "analysis_df = pd.DataFrame(results)\n",
    "\n",
    "df_enriched = df.copy()\n",
    "df_enriched['article_index'] = df_enriched.index\n",
    "\n",
    "df_enriched = df_enriched.merge(\n",
    "    analysis_df,\n",
    "    on='article_index',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Enriched dataset shape: {df_enriched.shape}\")\n",
    "print(f\"New columns added: {[col for col in analysis_df.columns if col != 'article_index']}\")\n",
    "\n",
    "print(\"\\nSample enriched article:\")\n",
    "sample = df_enriched.iloc[0]\n",
    "print(f\"Title: {sample['title']}\")\n",
    "print(f\"Market: {sample['market_name']}\")\n",
    "print(f\"Relevance: {sample['relevance_score']}/10\")\n",
    "print(f\"Impact: {sample['impact_level']}\")\n",
    "print(f\"Category: {sample['category']}\")\n",
    "print(f\"What happened: {sample['what_happened']}\")\n",
    "\n",
    "print(\"\\nMerge successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0f63b",
   "metadata": {},
   "source": [
    "### **Validating Analysis Results**\n",
    "\n",
    "**What I am doing:**\n",
    "- Check distribution of relevance scores across markets\n",
    "- Count articles by category and impact level\n",
    "- Identify high-relevance articles for dashboard spotlight\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- Ensure LLM analysis is reasonable (not all 10s or all 0s)\n",
    "- Understand category distribution for visualization planning\n",
    "- Find top articles to feature prominently in dashboard\n",
    "\n",
    "**Technical Note:** pandas value_counts, groupby, filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nRelevance Score Distribution:\")\n",
    "print(df_enriched['relevance_score'].describe())\n",
    "\n",
    "print(f\"\\nScore breakdown:\")\n",
    "print(df_enriched['relevance_score'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7575ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nArticles by Category:\")\n",
    "print(df_enriched['category'].value_counts())\n",
    "\n",
    "print(\"\\nArticles by Impact Level:\")\n",
    "print(df_enriched['impact_level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAverage Relevance by Market:\")\n",
    "print(df_enriched.groupby('market_name')['relevance_score'].mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nHigh-Impact Articles (score >= 7):\")\n",
    "high_relevance = df_enriched[df_enriched['relevance_score'] >= 7]\n",
    "print(f\"Count: {len(high_relevance)} ({len(high_relevance)/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTop 5 Highest Scored Articles:\")\n",
    "top_articles = df_enriched.nlargest(5, 'relevance_score')[['title', 'market_name', 'relevance_score', 'category']]\n",
    "for idx, row in top_articles.iterrows():\n",
    "    print(f\"\\n[{row['relevance_score']}/10] {row['title'][:70]}...\")\n",
    "    print(f\"  Market: {row['market_name']} | Category: {row['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a88a9",
   "metadata": {},
   "source": [
    "### **Saving Data-Processed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf49cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = output_dir / f'analyzed_articles_{timestamp}.json'\n",
    "df_enriched.to_json(output_file, orient='records', date_format='iso', indent=2)\n",
    "\n",
    "print(f\"Saved to: {output_file}\")\n",
    "print(f\"Total articles analyzed: {len(df_enriched)}\")\n",
    "\n",
    "summary = {\n",
    "    'analysis_timestamp': timestamp,\n",
    "    'total_articles': len(df_enriched),\n",
    "    'llm_model': config['llm']['model'],\n",
    "    'relevance_stats': {\n",
    "        'mean': float(df_enriched['relevance_score'].mean()),\n",
    "        'median': float(df_enriched['relevance_score'].median()),\n",
    "        'high_relevance_count': int((df_enriched['relevance_score'] >= 7).sum())\n",
    "    },\n",
    "    'categories': df_enriched['category'].value_counts().to_dict(),\n",
    "    'impact_levels': df_enriched['impact_level'].value_counts().to_dict(),\n",
    "    'by_market': df_enriched.groupby('market_name')['relevance_score'].agg(['count', 'mean']).to_dict()\n",
    "}\n",
    "\n",
    "summary_file = output_dir / f'analysis_summary_{timestamp}.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary saved to: {summary_file}\")\n",
    "print(\"\\nNotebook 02 complete! Ready for visualization in Notebook 03.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
