{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd016547",
   "metadata": {},
   "source": [
    "\n",
    "I found \n",
    "\n",
    "\n",
    "| Country        | Value (NOK)           | Investments | % of Portfolio |\n",
    "|----------------|-----------------------|-------------|----------------|\n",
    "| USA            | 10,488,258,386,469   | 2,901       | 52.4%          |\n",
    "| Japan          | 1,207,846,613,870    | 1,410       | 6.0%           |\n",
    "| United Kingdom | 1,100,973,152,288    | 1,036       | 5.5%           |\n",
    "| Germany        | 916,292,718,569      | 295         | 4.6%           |\n",
    "| France         | 708,716,940,392      | 263         | 3.5%           |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d20ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: newsapi in c:\\users\\dhruv\\appdata\\roaming\\python\\python312\\site-packages (0.1.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from newsapi) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->newsapi) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->newsapi) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->newsapi) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->newsapi) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newsapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d466383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import feedparser\n",
    "from newsapi import NewsApiClient\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb291dc",
   "metadata": {},
   "source": [
    "### **Loading Configuration**\n",
    "\n",
    "**What I am doing:**\n",
    "- Loading environment variables from .env file (API keys)\n",
    "- Reading config.yaml for market definitions and parameters\n",
    "- Validating that required API keys are present\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- Secure credential management (keys not hardcoded)\n",
    "- Centralized configuration for easy updates\n",
    "- Early validation prevents runtime errors later\n",
    "\n",
    "**Technical Note:** dotenv for environment variables, yaml for configuration parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efdd53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsAPI key loaded\n",
      "OpenAI key loaded\n",
      "Config loaded: 5 markets\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "NEWS_API_KEY = os.getenv('NEWS_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not NEWS_API_KEY or NEWS_API_KEY == 'your_newsapi_key_here':\n",
    "    print(\"WARNING: NewsAPI key missing\")\n",
    "else:\n",
    "    print(\"NewsAPI key loaded\")\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY == 'your_openai_key_here':\n",
    "    print(\"WARNING: OpenAI key missing\")\n",
    "else:\n",
    "    print(\"OpenAI key loaded\")\n",
    "\n",
    "print(f\"Config loaded: {len(config['markets'])} markets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1bf8e",
   "metadata": {},
   "source": [
    "### **Data Source Functions**\n",
    "\n",
    "What I am doing:\n",
    "\n",
    "Creating function to fetch from NewsAPI with keyword queries\n",
    "Creating function to fetch from Google News RSS feeds\n",
    "Adding metadata (market_id, source_type) to each article\n",
    "\n",
    "Why I'm doing this:\n",
    "\n",
    "Modular design allows easy testing and debugging\n",
    "Multiple data sources provide broader coverage\n",
    "Metadata enables tracking and filtering by source\n",
    "\n",
    "Technical Note: newsapi-python library, feedparser for RSS, requests for HTTP calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c674a936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source functions defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_newsapi(market_id, market_config, days_back=30):\n",
    "    \"\"\"Fetch news from NewsAPI\"\"\"\n",
    "    try:\n",
    "        newsapi = NewsApiClient(api_key=NEWS_API_KEY)\n",
    "        \n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days_back)\n",
    "        \n",
    "        query = ' OR '.join([f'\"{kw}\"' for kw in market_config['keywords']])\n",
    "        \n",
    "        response = newsapi.get_everything(\n",
    "            q=query,\n",
    "            from_param=start_date.strftime('%Y-%m-%d'),\n",
    "            to=end_date.strftime('%Y-%m-%d'),\n",
    "            language='en',\n",
    "            sort_by='publishedAt',\n",
    "            page_size=100\n",
    "        )\n",
    "        \n",
    "        if response['status'] == 'ok':\n",
    "            articles = response['articles']\n",
    "            for article in articles:\n",
    "                article['source_type'] = 'newsapi'\n",
    "                article['market_id'] = market_id\n",
    "                article['market_name'] = market_config['name']\n",
    "            return articles\n",
    "        return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"NewsAPI error for {market_config['name']}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def fetch_google_news_rss(market_id, market_config):\n",
    "    \"\"\"Fetch news from Google News RSS\"\"\"\n",
    "    try:\n",
    "        articles = []\n",
    "        for keyword in market_config['keywords'][:3]:\n",
    "            query = keyword.replace(' ', '+')\n",
    "            rss_url = f\"https://news.google.com/rss/search?q={query}+when:30d&hl=en&gl={market_config['country_code']}&ceid={market_config['country_code']}:en\"\n",
    "            \n",
    "            feed = feedparser.parse(rss_url)\n",
    "            \n",
    "            for entry in feed.entries[:20]:\n",
    "                if hasattr(entry, 'published_parsed') and entry.published_parsed:\n",
    "                    pub_date = datetime(*entry.published_parsed[:6])\n",
    "                else:\n",
    "                    pub_date = datetime.now()\n",
    "                \n",
    "                article = {\n",
    "                    'title': entry.get('title', ''),\n",
    "                    'description': entry.get('summary', ''),\n",
    "                    'url': entry.get('link', ''),\n",
    "                    'publishedAt': pub_date.isoformat(),\n",
    "                    'source': {'name': entry.get('source', {}).get('title', 'Google News')},\n",
    "                    'source_type': 'google_news',\n",
    "                    'market_id': market_id,\n",
    "                    'market_name': market_config['name']\n",
    "                }\n",
    "                articles.append(article)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        return articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Google News error for {market_config['name']}: {e}\")\n",
    "        return []\n",
    "print(\"Data source functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e312161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entry structure:\n",
      "Title: SEC Football Players of the Week: Oct. 27 - Southeastern Conference\n",
      "Published field: Mon, 27 Oct 2025 19:43:14 GMT\n",
      "Published_parsed: time.struct_time(tm_year=2025, tm_mon=10, tm_mday=27, tm_hour=19, tm_min=43, tm_sec=14, tm_wday=0, tm_yday=300, tm_isdst=0)\n",
      "\n",
      "All keys: dict_keys(['title', 'title_detail', 'links', 'link', 'id', 'guidislink', 'published', 'published_parsed', 'summary', 'summary_detail', 'source'])\n"
     ]
    }
   ],
   "source": [
    "# Debug Google News date parsing\n",
    "test_feed = feedparser.parse(\"https://news.google.com/rss/search?q=SEC+when:30d&hl=en&gl=us&ceid=us:en\")\n",
    "\n",
    "if test_feed.entries:\n",
    "    sample = test_feed.entries[0]\n",
    "    print(\"Sample entry structure:\")\n",
    "    print(f\"Title: {sample.get('title')}\")\n",
    "    print(f\"Published field: {sample.get('published')}\")\n",
    "    print(f\"Published_parsed: {sample.get('published_parsed')}\")\n",
    "    print(f\"\\nAll keys: {sample.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81b845",
   "metadata": {},
   "source": [
    "**What I am doing:**\n",
    "- Running both fetch functions on USA market only\n",
    "- Counting articles from each source\n",
    "- Displaying sample article to verify data structure\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- Validate functions work before full fetch (fail fast)\n",
    "- Check data quality and relevance\n",
    "- Estimate total article volume before processing all markets\n",
    "\n",
    "**Technical Note**: Basic Python testing, manual validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccc42485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data fetch for United States\n",
      "\n",
      "Fetching from NewsAPI...\n",
      "NewsAPI: 100 articles\n",
      "\n",
      "Fetching from Google News...\n",
      "Google News: 60 articles\n",
      "\n",
      "Total US articles: 160\n",
      "\n",
      "Sample article:\n",
      "Title: Fed official warns inflation is still too high for more rate cuts\n",
      "Source: Biztoc.com\n",
      "Type: newsapi\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing data fetch for United States\\n\")\n",
    "\n",
    "us_config = config['markets']['us']\n",
    "\n",
    "print(\"Fetching from NewsAPI...\")\n",
    "newsapi_articles = fetch_newsapi('us', us_config, days_back=30)\n",
    "print(f\"NewsAPI: {len(newsapi_articles)} articles\")\n",
    "\n",
    "print(\"\\nFetching from Google News...\")\n",
    "google_articles = fetch_google_news_rss('us', us_config)\n",
    "print(f\"Google News: {len(google_articles)} articles\")\n",
    "\n",
    "all_us_articles = newsapi_articles + google_articles\n",
    "print(f\"\\nTotal US articles: {len(all_us_articles)}\")\n",
    "\n",
    "if all_us_articles:\n",
    "    sample = all_us_articles[0]\n",
    "    print(\"\\nSample article:\")\n",
    "    print(f\"Title: {sample['title']}\")\n",
    "    print(f\"Source: {sample.get('source', {}).get('name', 'Unknown')}\")\n",
    "    print(f\"Type: {sample['source_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c957b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for all markets\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Markets:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing United States...\n",
      "  NewsAPI: 100 articles\n",
      "  Google News: 60 articles\n",
      "  Total for United States: 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Markets:  20%|██        | 1/5 [00:04<00:16,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Japan...\n",
      "  NewsAPI: 97 articles\n",
      "  Google News: 47 articles\n",
      "  Total for Japan: 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Markets:  40%|████      | 2/5 [00:09<00:14,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing United Kingdom...\n",
      "  NewsAPI: 81 articles\n",
      "  Google News: 60 articles\n",
      "  Total for United Kingdom: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Markets:  60%|██████    | 3/5 [00:14<00:10,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Germany...\n",
      "  NewsAPI: 40 articles\n",
      "  Google News: 60 articles\n",
      "  Total for Germany: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Markets:  80%|████████  | 4/5 [00:19<00:05,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing France...\n",
      "  NewsAPI: 87 articles\n",
      "  Google News: 53 articles\n",
      "  Total for France: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Markets: 100%|██████████| 5/5 [00:24<00:00,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Articles Collected: 685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching news for all markets\\n\")\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "for market_id, market_config in tqdm(config['markets'].items(), desc=\"Markets\"):\n",
    "    print(f\"\\nProcessing {market_config['name']}...\")\n",
    "    \n",
    "    newsapi_articles = fetch_newsapi(market_id, market_config, days_back=30)\n",
    "    print(f\"  NewsAPI: {len(newsapi_articles)} articles\")\n",
    "    \n",
    "    google_articles = fetch_google_news_rss(market_id, market_config)\n",
    "    print(f\"  Google News: {len(google_articles)} articles\")\n",
    "    \n",
    "    market_articles = newsapi_articles + google_articles\n",
    "    all_articles.extend(market_articles)\n",
    "    \n",
    "    print(f\"  Total for {market_config['name']}: {len(market_articles)}\")\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting between markets\n",
    "\n",
    "\n",
    "print(f\"Total Articles Collected: {len(all_articles)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b754ef",
   "metadata": {},
   "source": [
    "### **Time for Pandas**\n",
    "\n",
    "**What Im Doing**:\n",
    "- Now that Ive collected a bunch of articles from all five markets, its time to structure these\n",
    "- I will be using a pandas dataframe. Since this is a small scale project, with 685 initial articles, I am not using SQL \n",
    "- I will try to extract source names from the nested dictionary\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- DataFrame enables efficient data manipulation and analysis\n",
    "- Proper datetime parsing required for time-based filtering and visualization\n",
    "- Flattening nested structures simplifies downstream processing\n",
    "\n",
    "**Technical Note:** pandas for data manipulation, datetime parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "572aa6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (685, 11)\n",
      "Columns: ['source', 'author', 'title', 'description', 'url', 'urlToImage', 'publishedAt', 'content', 'source_type', 'market_id', 'market_name']\n",
      "\n",
      "DataFrame created: 685 rows, 12 columns\n",
      "Date range: 2025-10-03 01:04:26+00:00 to 2025-11-01 14:13:42+00:00\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>source_type</th>\n",
       "      <th>market_id</th>\n",
       "      <th>market_name</th>\n",
       "      <th>source_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': None, 'name': 'Biztoc.com'}</td>\n",
       "      <td>thestreet.com</td>\n",
       "      <td>Fed official warns inflation is still too high...</td>\n",
       "      <td>Federal Reserve officials are divided again ov...</td>\n",
       "      <td>https://biztoc.com/x/6a7fb82c9f09a310</td>\n",
       "      <td>https://biztoc.com/cdn/6a7fb82c9f09a310_s.webp</td>\n",
       "      <td>2025-11-01 14:13:42+00:00</td>\n",
       "      <td>Federal Reserve officials are divided again ov...</td>\n",
       "      <td>newsapi</td>\n",
       "      <td>us</td>\n",
       "      <td>United States</td>\n",
       "      <td>Biztoc.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': None, 'name': 'GlobeNewswire'}</td>\n",
       "      <td>The Rosen Law Firm PA</td>\n",
       "      <td>ROSEN, RECOGNIZED INVESTOR COUNSEL, Encourages...</td>\n",
       "      <td>NEW YORK, Nov. 01, 2025 (GLOBE NEWSWIRE) -- WH...</td>\n",
       "      <td>https://www.globenewswire.com/news-release/202...</td>\n",
       "      <td>https://ml.globenewswire.com/Resource/Download...</td>\n",
       "      <td>2025-11-01 14:09:00+00:00</td>\n",
       "      <td>NEW YORK, Nov. 01, 2025 (GLOBE NEWSWIRE) -- \\r...</td>\n",
       "      <td>newsapi</td>\n",
       "      <td>us</td>\n",
       "      <td>United States</td>\n",
       "      <td>GlobeNewswire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': None, 'name': 'CBS Sports'}</td>\n",
       "      <td>Owen OBrien</td>\n",
       "      <td>Use DraftKings promo code to get $300 bonus be...</td>\n",
       "      <td>DraftKings offers $300 in bonus bets if your f...</td>\n",
       "      <td>https://www.cbssports.com/college-football/new...</td>\n",
       "      <td>https://sportshub.cbsistatic.com/i/r/2025/10/1...</td>\n",
       "      <td>2025-11-01 14:06:04+00:00</td>\n",
       "      <td>New users can capitalize on the latest DraftKi...</td>\n",
       "      <td>newsapi</td>\n",
       "      <td>us</td>\n",
       "      <td>United States</td>\n",
       "      <td>CBS Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': None, 'name': 'Crooksandliars.com'}</td>\n",
       "      <td>John Amato</td>\n",
       "      <td>Newsmax Host Bravely Resurrects Reagan-Era Wel...</td>\n",
       "      <td>Newsmax host Rob Schmidt called SNAP an \"ugly ...</td>\n",
       "      <td>https://crooksandliars.com/2025/10/newsmax-hos...</td>\n",
       "      <td>https://crooksandliars.com/files/mediaposters/...</td>\n",
       "      <td>2025-11-01 14:03:03+00:00</td>\n",
       "      <td>Newsmax host Rob Schmidt called SNAP an \"ugly ...</td>\n",
       "      <td>newsapi</td>\n",
       "      <td>us</td>\n",
       "      <td>United States</td>\n",
       "      <td>Crooksandliars.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': 'usa-today', 'name': 'USA Today'}</td>\n",
       "      <td>Vols Wire</td>\n",
       "      <td>Tennessee football announces game captains aga...</td>\n",
       "      <td>Tennessee football announces game captains aga...</td>\n",
       "      <td>https://volswire.usatoday.com/story/sports/col...</td>\n",
       "      <td>https://s.yimg.com/ny/api/res/1.2/7F4A.V52DWyF...</td>\n",
       "      <td>2025-11-01 14:02:07+00:00</td>\n",
       "      <td>No. 18 Oklahoma (6-2, 2-2 SEC) will travel to ...</td>\n",
       "      <td>newsapi</td>\n",
       "      <td>us</td>\n",
       "      <td>United States</td>\n",
       "      <td>USA Today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       source                 author  \\\n",
       "0          {'id': None, 'name': 'Biztoc.com'}          thestreet.com   \n",
       "1       {'id': None, 'name': 'GlobeNewswire'}  The Rosen Law Firm PA   \n",
       "2          {'id': None, 'name': 'CBS Sports'}            Owen OBrien   \n",
       "3  {'id': None, 'name': 'Crooksandliars.com'}             John Amato   \n",
       "4    {'id': 'usa-today', 'name': 'USA Today'}              Vols Wire   \n",
       "\n",
       "                                               title  \\\n",
       "0  Fed official warns inflation is still too high...   \n",
       "1  ROSEN, RECOGNIZED INVESTOR COUNSEL, Encourages...   \n",
       "2  Use DraftKings promo code to get $300 bonus be...   \n",
       "3  Newsmax Host Bravely Resurrects Reagan-Era Wel...   \n",
       "4  Tennessee football announces game captains aga...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Federal Reserve officials are divided again ov...   \n",
       "1  NEW YORK, Nov. 01, 2025 (GLOBE NEWSWIRE) -- WH...   \n",
       "2  DraftKings offers $300 in bonus bets if your f...   \n",
       "3  Newsmax host Rob Schmidt called SNAP an \"ugly ...   \n",
       "4  Tennessee football announces game captains aga...   \n",
       "\n",
       "                                                 url  \\\n",
       "0              https://biztoc.com/x/6a7fb82c9f09a310   \n",
       "1  https://www.globenewswire.com/news-release/202...   \n",
       "2  https://www.cbssports.com/college-football/new...   \n",
       "3  https://crooksandliars.com/2025/10/newsmax-hos...   \n",
       "4  https://volswire.usatoday.com/story/sports/col...   \n",
       "\n",
       "                                          urlToImage  \\\n",
       "0     https://biztoc.com/cdn/6a7fb82c9f09a310_s.webp   \n",
       "1  https://ml.globenewswire.com/Resource/Download...   \n",
       "2  https://sportshub.cbsistatic.com/i/r/2025/10/1...   \n",
       "3  https://crooksandliars.com/files/mediaposters/...   \n",
       "4  https://s.yimg.com/ny/api/res/1.2/7F4A.V52DWyF...   \n",
       "\n",
       "                publishedAt  \\\n",
       "0 2025-11-01 14:13:42+00:00   \n",
       "1 2025-11-01 14:09:00+00:00   \n",
       "2 2025-11-01 14:06:04+00:00   \n",
       "3 2025-11-01 14:03:03+00:00   \n",
       "4 2025-11-01 14:02:07+00:00   \n",
       "\n",
       "                                             content source_type market_id  \\\n",
       "0  Federal Reserve officials are divided again ov...     newsapi        us   \n",
       "1  NEW YORK, Nov. 01, 2025 (GLOBE NEWSWIRE) -- \\r...     newsapi        us   \n",
       "2  New users can capitalize on the latest DraftKi...     newsapi        us   \n",
       "3  Newsmax host Rob Schmidt called SNAP an \"ugly ...     newsapi        us   \n",
       "4  No. 18 Oklahoma (6-2, 2-2 SEC) will travel to ...     newsapi        us   \n",
       "\n",
       "     market_name         source_name  \n",
       "0  United States          Biztoc.com  \n",
       "1  United States       GlobeNewswire  \n",
       "2  United States          CBS Sports  \n",
       "3  United States  Crooksandliars.com  \n",
       "4  United States           USA Today  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_articles)\n",
    "\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "df['publishedAt'] = pd.to_datetime(df['publishedAt'], errors='coerce')\n",
    "\n",
    "df['source_name'] = df['source'].apply(\n",
    "    lambda x: x.get('name', 'Unknown') if isinstance(x, dict) else 'Unknown'\n",
    ")\n",
    "\n",
    "print(f\"\\nDataFrame created: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Date range: {df['publishedAt'].min()} to {df['publishedAt'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9459202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im getting rif of that messy looking source column, I already have a separate source_name column\n",
    "df = df.drop(columns=['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a107133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Assessment\n",
      "\n",
      "\n",
      "Missing Values:\n",
      "title            0\n",
      "description      0\n",
      "url              0\n",
      "publishedAt    271\n",
      "market_id        0\n",
      "dtype: int64\n",
      "\n",
      "Articles by Market:\n",
      "market_name\n",
      "United States     160\n",
      "Japan             142\n",
      "France            129\n",
      "United Kingdom    122\n",
      "Germany            99\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Articles by Source Type:\n",
      "source_type\n",
      "newsapi        381\n",
      "google_news    271\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Quality Assessment\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df[['title', 'description', 'url', 'publishedAt', 'market_id']].isnull().sum())\n",
    "\n",
    "print(\"\\nArticles by Market:\")\n",
    "print(df['market_name'].value_counts())\n",
    "\n",
    "print(\"\\nArticles by Source Type:\")\n",
    "print(df['source_type'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad8fe122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Articles by Source Name (Top 10):\n",
      "source_name\n",
      "Biztoc.com             48\n",
      "Yahoo Entertainment    36\n",
      "The Times of India     31\n",
      "GlobeNewswire          29\n",
      "Reuters                25\n",
      "Bloomberg.com          17\n",
      "Bank of England        14\n",
      "CNA                    14\n",
      "USA Today              14\n",
      "Financial Post         13\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Date Distribution:\n",
      "publishedAt\n",
      "2025-10-23      3\n",
      "2025-10-24      1\n",
      "2025-10-25      3\n",
      "2025-10-26      3\n",
      "2025-10-27      7\n",
      "2025-10-28      7\n",
      "2025-10-29     14\n",
      "2025-10-30     96\n",
      "2025-10-31     69\n",
      "2025-11-01    111\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Articles by Source Name (Top 10):\")\n",
    "print(df['source_name'].value_counts().head(10))\n",
    "\n",
    "print(\"\\n Date Distribution:\")\n",
    "print(df['publishedAt'].dt.date.value_counts().sort_index().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0403c03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication\n",
      "\n",
      "Articles before deduplication: 652\n",
      "Duplicates found: 0\n",
      "Articles after deduplication: 652\n",
      "Removed: 0 duplicate articles\n",
      "\n",
      "Final distribution by market:\n",
      "market_name\n",
      "United States     160\n",
      "Japan             142\n",
      "France            129\n",
      "United Kingdom    122\n",
      "Germany            99\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Deduplication\\n\")\n",
    "\n",
    "print(f\"Articles before deduplication: {len(df)}\")\n",
    "\n",
    "duplicates = df.duplicated(subset=['title', 'url'], keep='first').sum()\n",
    "print(f\"Duplicates found: {duplicates}\")\n",
    "\n",
    "df = df.drop_duplicates(subset=['title', 'url'], keep='first')\n",
    "\n",
    "print(f\"Articles after deduplication: {len(df)}\")\n",
    "print(f\"Removed: {duplicates} duplicate articles\")\n",
    "\n",
    "print(\"\\nFinal distribution by market:\")\n",
    "print(df['market_name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb86bb5d",
   "metadata": {},
   "source": [
    "**What I am doing:**\n",
    "- Creating timestamp for file versioning\n",
    "- Saving deduplicated DataFrame to JSON in data/raw directory\n",
    "- Generating summary statistics file for reference\n",
    "\n",
    "**Why I'm doing this:**\n",
    "- Preserves raw collected data before LLM analysis\n",
    "- Timestamped files allow tracking multiple collection runs\n",
    "- JSON format enables easy loading in next notebook\n",
    "\n",
    "**Technical Note:** pandas to_json, Path for file handling, json for metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1162964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving raw data\n",
      "\n",
      "Saved to: ..\\data\\raw\\news_raw_20251102_195719.json\n",
      "Total articles: 652\n",
      "\n",
      "Summary saved to: ..\\data\\raw\\collection_summary_20251102_195719.json\n",
      "\n",
      "Collection complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving raw data\\n\")\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = Path('../data/raw')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = output_dir / f'news_raw_{timestamp}.json'\n",
    "df.to_json(output_file, orient='records', date_format='iso', indent=2)\n",
    "\n",
    "print(f\"Saved to: {output_file}\")\n",
    "print(f\"Total articles: {len(df)}\")\n",
    "\n",
    "summary = {\n",
    "    'collection_timestamp': timestamp,\n",
    "    'total_articles': len(df),\n",
    "    'date_range': {\n",
    "        'start': df['publishedAt'].min().isoformat() if pd.notna(df['publishedAt'].min()) else None,\n",
    "        'end': df['publishedAt'].max().isoformat() if pd.notna(df['publishedAt'].max()) else None\n",
    "    },\n",
    "    'articles_by_market': df['market_name'].value_counts().to_dict(),\n",
    "    'articles_by_source': df['source_type'].value_counts().to_dict(),\n",
    "    'missing_dates': int(df['publishedAt'].isna().sum())\n",
    "}\n",
    "\n",
    "summary_file = output_dir / f'collection_summary_{timestamp}.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary saved to: {summary_file}\")\n",
    "print(\"\\nCollection complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
